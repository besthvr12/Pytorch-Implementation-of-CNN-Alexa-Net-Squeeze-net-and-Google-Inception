{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alexa Net using Keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uoBYTzLOtPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "np.random.seed(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRbBT89UQMYR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d310afbd-1ac0-48ef-8739-435c53595499"
      },
      "source": [
        "# (1) Importing dependency\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten,\\\n",
        " Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import numpy as np\n",
        "np.random.seed(1000)\n",
        "\n",
        "# (2) Get Data\n",
        "import tflearn.datasets.oxflower17 as oxflower17\n",
        "x, y = oxflower17.load_data(one_hot=True)\n",
        "\n",
        "# (3) Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),\\\n",
        " strides=(4,4), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling \n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation before passing it to the next layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Dense Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 3rd Dense Layer\n",
        "model.add(Dense(1000))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(17))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# (4) Compile \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',\\\n",
        " metrics=['accuracy'])\n",
        "\n",
        "# (5) Train\n",
        "model.fit(x, y, batch_size=64, epochs=40, verbose=1, \\\n",
        "validation_split=0.2, shuffle=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 54, 54, 96)        34944     \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 54, 54, 96)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 27, 27, 96)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 27, 27, 96)        384       \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 17, 17, 256)       2973952   \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 17, 17, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 6, 6, 384)         885120    \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 6, 6, 384)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 6, 6, 384)         1536      \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 4, 4, 384)         1327488   \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 4, 4, 384)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 4, 4, 384)         1536      \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 2, 2, 256)         884992    \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 1, 1, 256)         1024      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 4096)              1052672   \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1000)              4097000   \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 17)                17017     \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 17)                0         \n",
            "=================================================================\n",
            "Total params: 28,096,769\n",
            "Trainable params: 28,075,633\n",
            "Non-trainable params: 21,136\n",
            "_________________________________________________________________\n",
            "Train on 1088 samples, validate on 272 samples\n",
            "Epoch 1/40\n",
            "1088/1088 [==============================] - 174s 160ms/step - loss: 2.9803 - acc: 0.2344 - val_loss: 12.5058 - val_acc: 0.1360\n",
            "Epoch 2/40\n",
            "1088/1088 [==============================] - 170s 156ms/step - loss: 2.1516 - acc: 0.3594 - val_loss: 10.0492 - val_acc: 0.1728\n",
            "Epoch 3/40\n",
            "1088/1088 [==============================] - 172s 158ms/step - loss: 1.8533 - acc: 0.4118 - val_loss: 5.3345 - val_acc: 0.3088\n",
            "Epoch 4/40\n",
            "1088/1088 [==============================] - 170s 156ms/step - loss: 1.6813 - acc: 0.4577 - val_loss: 3.1996 - val_acc: 0.3529\n",
            "Epoch 5/40\n",
            "1088/1088 [==============================] - 171s 157ms/step - loss: 1.4652 - acc: 0.5037 - val_loss: 3.0905 - val_acc: 0.4081\n",
            "Epoch 6/40\n",
            "1088/1088 [==============================] - 170s 156ms/step - loss: 1.3981 - acc: 0.5267 - val_loss: 3.6115 - val_acc: 0.3640\n",
            "Epoch 7/40\n",
            "1088/1088 [==============================] - 170s 156ms/step - loss: 1.4046 - acc: 0.5551 - val_loss: 2.8825 - val_acc: 0.4154\n",
            "Epoch 8/40\n",
            "1088/1088 [==============================] - 170s 156ms/step - loss: 1.2143 - acc: 0.6204 - val_loss: 2.4024 - val_acc: 0.4412\n",
            "Epoch 9/40\n",
            "1088/1088 [==============================] - 170s 157ms/step - loss: 1.1033 - acc: 0.6333 - val_loss: 2.2685 - val_acc: 0.4265\n",
            "Epoch 10/40\n",
            "1088/1088 [==============================] - 171s 157ms/step - loss: 0.9702 - acc: 0.6893 - val_loss: 1.8493 - val_acc: 0.5074\n",
            "Epoch 11/40\n",
            "1088/1088 [==============================] - 170s 156ms/step - loss: 0.8898 - acc: 0.7086 - val_loss: 2.3085 - val_acc: 0.4779\n",
            "Epoch 12/40\n",
            "1088/1088 [==============================] - 168s 154ms/step - loss: 0.8142 - acc: 0.7243 - val_loss: 2.4516 - val_acc: 0.4559\n",
            "Epoch 13/40\n",
            "1088/1088 [==============================] - 166s 152ms/step - loss: 0.7528 - acc: 0.7426 - val_loss: 2.7446 - val_acc: 0.4926\n",
            "Epoch 14/40\n",
            "1088/1088 [==============================] - 166s 152ms/step - loss: 0.7262 - acc: 0.7509 - val_loss: 2.7197 - val_acc: 0.4779\n",
            "Epoch 15/40\n",
            "1088/1088 [==============================] - 164s 151ms/step - loss: 0.6542 - acc: 0.7757 - val_loss: 2.6175 - val_acc: 0.4706\n",
            "Epoch 16/40\n",
            "1088/1088 [==============================] - 164s 150ms/step - loss: 0.5231 - acc: 0.8180 - val_loss: 2.4405 - val_acc: 0.4743\n",
            "Epoch 17/40\n",
            "1088/1088 [==============================] - 165s 151ms/step - loss: 0.5392 - acc: 0.8217 - val_loss: 2.9702 - val_acc: 0.4596\n",
            "Epoch 18/40\n",
            "1088/1088 [==============================] - 164s 151ms/step - loss: 0.5246 - acc: 0.8254 - val_loss: 3.5138 - val_acc: 0.4081\n",
            "Epoch 19/40\n",
            "1088/1088 [==============================] - 165s 151ms/step - loss: 0.4993 - acc: 0.8263 - val_loss: 3.0130 - val_acc: 0.4485\n",
            "Epoch 20/40\n",
            "1088/1088 [==============================] - 165s 152ms/step - loss: 0.4255 - acc: 0.8631 - val_loss: 2.9504 - val_acc: 0.4926\n",
            "Epoch 21/40\n",
            "1088/1088 [==============================] - 166s 152ms/step - loss: 0.3244 - acc: 0.8879 - val_loss: 3.1068 - val_acc: 0.4485\n",
            "Epoch 22/40\n",
            "1088/1088 [==============================] - 165s 152ms/step - loss: 0.2974 - acc: 0.9053 - val_loss: 2.8496 - val_acc: 0.5147\n",
            "Epoch 23/40\n",
            "1088/1088 [==============================] - 165s 152ms/step - loss: 0.2669 - acc: 0.9191 - val_loss: 3.5837 - val_acc: 0.4632\n",
            "Epoch 24/40\n",
            "1088/1088 [==============================] - 166s 152ms/step - loss: 0.2504 - acc: 0.9081 - val_loss: 3.5536 - val_acc: 0.4412\n",
            "Epoch 25/40\n",
            "1088/1088 [==============================] - 165s 151ms/step - loss: 0.2408 - acc: 0.9228 - val_loss: 3.6298 - val_acc: 0.4449\n",
            "Epoch 26/40\n",
            "1088/1088 [==============================] - 166s 152ms/step - loss: 0.2116 - acc: 0.9301 - val_loss: 2.9372 - val_acc: 0.5551\n",
            "Epoch 27/40\n",
            "1088/1088 [==============================] - 165s 152ms/step - loss: 0.2329 - acc: 0.9265 - val_loss: 3.5042 - val_acc: 0.4890\n",
            "Epoch 28/40\n",
            "1088/1088 [==============================] - 165s 152ms/step - loss: 0.1972 - acc: 0.9421 - val_loss: 2.8293 - val_acc: 0.4926\n",
            "Epoch 29/40\n",
            "1088/1088 [==============================] - 165s 152ms/step - loss: 0.2395 - acc: 0.9237 - val_loss: 3.2800 - val_acc: 0.5074\n",
            "Epoch 30/40\n",
            "1088/1088 [==============================] - 165s 152ms/step - loss: 0.2494 - acc: 0.9164 - val_loss: 3.1876 - val_acc: 0.5000\n",
            "Epoch 31/40\n",
            "1088/1088 [==============================] - 164s 151ms/step - loss: 0.1660 - acc: 0.9439 - val_loss: 3.0352 - val_acc: 0.5110\n",
            "Epoch 32/40\n",
            "1088/1088 [==============================] - 164s 151ms/step - loss: 0.1499 - acc: 0.9449 - val_loss: 3.3313 - val_acc: 0.4706\n",
            "Epoch 33/40\n",
            "1088/1088 [==============================] - 165s 151ms/step - loss: 0.1539 - acc: 0.9485 - val_loss: 3.1890 - val_acc: 0.5184\n",
            "Epoch 34/40\n",
            "1088/1088 [==============================] - 163s 150ms/step - loss: 0.1696 - acc: 0.9458 - val_loss: 3.6518 - val_acc: 0.4926\n",
            "Epoch 35/40\n",
            "1088/1088 [==============================] - 164s 151ms/step - loss: 0.2222 - acc: 0.9292 - val_loss: 3.6075 - val_acc: 0.4816\n",
            "Epoch 36/40\n",
            "1088/1088 [==============================] - 165s 151ms/step - loss: 0.2050 - acc: 0.9256 - val_loss: 3.4379 - val_acc: 0.4853\n",
            "Epoch 37/40\n",
            "1088/1088 [==============================] - 165s 152ms/step - loss: 0.2115 - acc: 0.9301 - val_loss: 3.4954 - val_acc: 0.5000\n",
            "Epoch 38/40\n",
            "1088/1088 [==============================] - 165s 151ms/step - loss: 0.1202 - acc: 0.9642 - val_loss: 2.9772 - val_acc: 0.5147\n",
            "Epoch 39/40\n",
            "1088/1088 [==============================] - 165s 152ms/step - loss: 0.1008 - acc: 0.9660 - val_loss: 3.1655 - val_acc: 0.5221\n",
            "Epoch 40/40\n",
            "1088/1088 [==============================] - 166s 152ms/step - loss: 0.0772 - acc: 0.9743 - val_loss: 3.0065 - val_acc: 0.5441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0ad863fb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "022qfgtahUUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}